<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Ellen Da-eun Lee </title> <meta name="author" content="Ellen Da-eun Lee"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://leedaeuni.github.io/publications/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ellen </span> Da-eun  Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kdd23-480.webp 480w,/assets/img/publication_preview/kdd23-800.webp 800w,/assets/img/publication_preview/kdd23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/kdd23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kdd23.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2023towards" class="col-sm-8"> <div class="title">Towards Suicide Prevention from Bipolar Disorder with Temporal Symptom-Aware Multitask Learning</div> <div class="author"> <em>Daeun Lee</em> ,  Sejung Son ,  Hyolim Jeon ,  Seungbae Kim ,  and  Jinyoung Han** </div> <div class="periodical"> <em>In ACM SIGKDD</em> , Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599917" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/DSAIL-SKKU/Temporal-Symptom-Aware-Multitask-Learning-KDD23" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Bipolar disorder (BD) is closely associated with an increased risk of suicide. However, while the prior work has revealed valuable insight into understanding the behavior of BD patients on social media, little attention has been paid to developing a model that can predict the future suicidality of a BD patient. Therefore, this study proposes a multi-task learning model for predicting the future suicidality of BD patients by jointly learning current symptoms. We build a novel BD dataset clinically validated by psychiatrists, including 14 years of posts on bipolar-related subreddits written by 818 BD patients, along with the annotations of future suicidality and BD symptoms. We also suggest a temporal symptom-aware attention mechanism to determine which symptoms are the most influential for predicting future suicidality over time through a sequence of BD posts. Our experiments demonstrate that the proposed model outperforms the state-of-the-art models in both BD symptom identification and future suicidality prediction tasks. In addition, the proposed temporal symptom-aware attention provides interpretable attention weights, helping clinicians to apprehend BD patients more comprehensively and to provide timely intervention by tracking mental state progression.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/human23-480.webp 480w,/assets/img/publication_preview/human23-800.webp 800w,/assets/img/publication_preview/human23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/human23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="human23.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="min2023detecting" class="col-sm-8"> <div class="title">Detecting depression on video logs using audiovisual features</div> <div class="author"> Kyungeun Min ,  Jeewoo Yoon ,  Migyeong Kang ,  <em>Daeun Lee</em> ,  Eunil Park , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jinyoung Han**' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Humanities and Social Sciences Communications</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.nature.com/articles/s41599-023-02313-6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Detecting depression on social media has received significant attention. Developing a depression detection model helps screen depressed individuals who may need proper treatment. While prior work mainly focused on developing depression detection models with social media posts, including text and image, little attention has been paid to how videos on social media can be used to detect depression. To this end, we propose a depression detection model that utilizes both audio and video features extracted from the vlogs (video logs) on YouTube. We first collected vlogs from YouTube and annotated them into depression and non-depression. We then analyze the statistical differences between depression and non-depression vlogs. Based on the lessons learned, we build a depression detection model that learns both audio and visual features, achieving high accuracy. We believe our model helps detect depressed individuals on social media at an early stage so that individuals who may need appropriate treatment can get help.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emnlp23-480.webp 480w,/assets/img/publication_preview/emnlp23-800.webp 800w,/assets/img/publication_preview/emnlp23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/emnlp23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emnlp23.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2023learning" class="col-sm-8"> <div class="title">Learning Co-Speech Gesture for Multimodal Aphasia Type Detection</div> <div class="author"> <em>Daeun Lee*</em> ,  Sejung Son* ,  Hyolim Jeon ,  Seungbae Kim ,  and  Jinyoung Han** </div> <div class="periodical"> <em>In EMNLP</em> , Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2023.emnlp-main.577/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/DSAIL-SKKU/Multimodal-Aphasia-Type-Detection_EMNLP_2023" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca’s and Wernicke’s aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We provide the codes for reproducibility purposes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tts23-480.webp 480w,/assets/img/publication_preview/tts23-800.webp 800w,/assets/img/publication_preview/tts23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tts23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tts23.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kang2023toward" class="col-sm-8"> <div class="title">Toward Natural and Intelligible Speech Synthesis: An Empirical Study on Transfer Learning</div> <div class="author"> Chaewon Kang ,  Jeewoo Yoon ,  <em>Daeun Lee</em> ,  Migyeong Kang ,  Seohyun Lim , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Juho Jung, Sejung Son, Jinyoung Han**' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In The Korean Institute of Broadcast and Media Engineers</em> , Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11514289" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>To synthesize natural and intelligible speech with a small amount of data, transfer learning with well-maintained and pre-trained data has been known to be useful. However, little attention has been paid to answer the following research questions with empirically-grounded evidence, "How much pre-trained (source) speech data (e.g., 10 K utterances or 10 hours) used in transfer learning is enough for generating natural and intelligible speech?" and "For generating natural and intelligible speech, how much (target) speech data should at least be provided?", which are essential for the quality of speech synthesis. To answer these questions, this paper conducts extensive experiments on speech synthesis with multiple source and target data with different lengths, speakers, and languages. We show that intelligible and natural speech can be synthesized with only 500 utterances of target data using transfer learning. Our work also reveals that at least 5000 utterances of source pre-trained data are required to synthesize decent speech.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/clp22-480.webp 480w,/assets/img/publication_preview/clp22-800.webp 800w,/assets/img/publication_preview/clp22-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/clp22.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="clp22.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2022detecting" class="col-sm-8"> <div class="title">Detecting suicidality with a contextual graph neural network</div> <div class="author"> <em>Daeun Lee</em> ,  Migyeong Kang ,  Minji Kim ,  and  Jinyoung Han** </div> <div class="periodical"> <em>In CLPsych (NAACL workshop)</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.clpsych-1.10/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/DSAIL-SKKU/Detecting_Suicidality_Contextual_GNN_CLPsych_2022" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Discovering individuals’ suicidality on social media has become increasingly important. Many researchers have studied to detect suicidality by using a suicide dictionary. However, while prior work focused on matching a word in a post with a suicide dictionary without considering contexts, little attention has been paid to how the word can be associated with the suicide-related context. To address this problem, we propose a suicidality detection model based on a graph neural network to grasp the dynamic semantic information of the suicide vocabulary by learning the relations between a given post and words. The extensive evaluation demonstrates that the proposed model achieves higher performance than the state-of-the-art methods. We believe the proposed model has great utility in identifying the suicidality of individuals and hence preventing individuals from potential suicide risks at an early stage.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jmir-480.webp 480w,/assets/img/publication_preview/jmir-800.webp 800w,/assets/img/publication_preview/jmir-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/jmir.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="jmir.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2021machine" class="col-sm-8"> <div class="title">Machine learning for mental health in social media: Bibliometric study</div> <div class="author"> Jina Kim ,  <em>Daeun Lee</em> ,  and  Eunil Park** </div> <div class="periodical"> <em>Journal of Medical Internet Research</em>, Mar 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.jmir.org/2021/3/e24870/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Background: Social media platforms provide an easily accessible and time-saving communication approach for individuals with mental disorders compared to face-to-face meetings with medical providers. Recently, machine learning (ML)-based mental health exploration using large-scale social media data has attracted significant attention. Objective: We aimed to provide a bibliometric analysis and discussion on research trends of ML for mental health in social media. Methods: Publications addressing social media and ML in the field of mental health were retrieved from the Scopus and Web of Science databases. We analyzed the publication distribution to measure productivity on sources, countries, institutions, authors, and research subjects, and visualized the trends in this field using a keyword co-occurrence network. The research methodologies of previous studies with high citations are also thoroughly described. Results: We obtained a total of 565 relevant papers published from 2015 to 2020. In the last 5 years, the number of publications has demonstrated continuous growth with Lecture Notes in Computer Science and Journal of Medical Internet Research as the two most productive sources based on Scopus and Web of Science records. In addition, notable methodological approaches with data resources presented in high-ranking publications were investigated. Conclusions: The results of this study highlight continuous growth in this research area. Moreover, we retrieved three main discussion points from a comprehensive overview of highly cited publications that provide new in-depth directions for both researchers and practitioners.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/covid21-480.webp 480w,/assets/img/publication_preview/covid21-800.webp 800w,/assets/img/publication_preview/covid21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/covid21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="covid21.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2021covid" class="col-sm-8"> <div class="title">COVID-19 Korean fake news detection using named entity and user reproliferation information</div> <div class="author"> Chaewon Park ,  Jiwon Kang ,  <em>Daeun Lee</em> ,  Munyoung Lee ,  and  Jinyoung Han** </div> <div class="periodical"> <em>In Human and Language Technology</em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://koreascience.kr/article/CFKO202130060784847.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emnlp20-480.webp 480w,/assets/img/publication_preview/emnlp20-800.webp 800w,/assets/img/publication_preview/emnlp20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/emnlp20.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emnlp20.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2020cross" class="col-sm-8"> <div class="title">Cross-lingual suicidal-oriented word embedding toward suicide prevention</div> <div class="author"> <em>Daeun Lee</em> ,  Soyoung Park ,  Jiwon Kang ,  Daejin Choi ,  and  Jinyoung Han** </div> <div class="periodical"> <em>In EMNLP Findings</em> , May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2020.findings-emnlp.200/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/DSAIL-SKKU/Cross_Lingual_Suicidal_Oriented_Word_Embedding_EMNLP_Findings_2020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Early intervention for suicide risks with social media data has increasingly received great attention. Using a suicide dictionary created by mental health experts is one of the effective ways to detect suicidal ideation. However, little attention has been paid to validate whether and how the existing dictionaries for other languages (i.e., English and Chinese) can be used for predicting suicidal ideation for a low-resource language (i.e., Korean) where a knowledge-based suicide dictionary has not yet been developed. To this end, we propose a cross-lingual suicidal ideation detection model that can identify whether a given social media post includes suicidal ideation or not. To utilize the existing suicide dictionaries developed for other languages (i.e., English and Chinese) in word embedding, our model translates a post written in the target language (i.e., Korean) into English and Chinese, and then uses the separate suicidal-oriented word embeddings developed for English and Chinese, respectively. By applying an ensemble approach for different languages, the model achieves high accuracy, over 87%. We believe our model is useful in accessing suicidal ideation using social media data for preventing potential suicide risk in an early stage.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ellen Da-eun Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@/imagesloaded.pkgd.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>